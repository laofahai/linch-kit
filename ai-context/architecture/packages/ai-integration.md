# @linch-kit/ai åŒ…æŠ€æœ¯æ–‡æ¡£

**åŒ…ç‰ˆæœ¬**: v1.0.0
**åˆ›å»ºæ—¥æœŸ**: 2025-06-23
**å¼€å‘ä¼˜å…ˆçº§**: P2 - ä¸­ä¼˜å…ˆçº§
**ä¾èµ–å…³ç³»**: core â†’ ai
**ç»´æŠ¤çŠ¶æ€**: ğŸ”„ å¼€å‘ä¸­

---

## ğŸ“– ç›®å½•

1. [æ¨¡å—æ¦‚è§ˆ](#1-æ¨¡å—æ¦‚è§ˆ)
2. [API è®¾è®¡](#2-api-è®¾è®¡)
3. [å®ç°ç»†èŠ‚](#3-å®ç°ç»†èŠ‚)
4. [é›†æˆæ¥å£](#4-é›†æˆæ¥å£)
5. [æœ€ä½³å®è·µ](#5-æœ€ä½³å®è·µ)
6. [æ€§èƒ½è€ƒé‡](#6-æ€§èƒ½è€ƒé‡)
7. [æµ‹è¯•ç­–ç•¥](#7-æµ‹è¯•ç­–ç•¥)
8. [AI é›†æˆæ”¯æŒ](#8-ai-é›†æˆæ”¯æŒ)

---

## 1. æ¨¡å—æ¦‚è§ˆ

### 1.1 åŠŸèƒ½å®šä½

@linch-kit/ai æ˜¯ LinchKit çš„ AI æœåŠ¡é›†æˆæ ¸å¿ƒåŒ…ï¼Œæä¾›ç»Ÿä¸€çš„ AI èƒ½åŠ›æŠ½è±¡å±‚ã€‚å®ƒæ”¯æŒå¤šç§ AI æœåŠ¡æä¾›å•†ï¼ˆOpenAIã€Anthropicã€æœ¬åœ°æ¨¡å‹ç­‰ï¼‰ï¼Œä¸ºæ•´ä¸ª LinchKit ç”Ÿæ€ç³»ç»Ÿæä¾›ä¸€è‡´çš„ AI è°ƒç”¨æ¥å£ã€‚

```mermaid
graph TB
    A[LinchKit åº”ç”¨å±‚] --> B[@linch-kit/ai]
    B --> C[AI æä¾›å•†æŠ½è±¡å±‚]
    C --> D[OpenAI Provider]
    C --> E[Anthropic Provider]
    C --> F[Local Model Provider]
    C --> G[Custom Provider]

    B --> H[AI æœåŠ¡ç®¡ç†å±‚]
    H --> I[æ³¨å†Œè¡¨ç®¡ç†]
    H --> J[é…ç½®ç®¡ç†]
    H --> K[ç¼“å­˜ç®¡ç†]
    H --> L[ç›‘æ§ç®¡ç†]
```

### 1.2 æ ¸å¿ƒä»·å€¼

- **ğŸ”Œ å¤šåç«¯æ”¯æŒ**: ç»Ÿä¸€æ¥å£æ”¯æŒä¸»æµ AI æœåŠ¡å’Œæœ¬åœ°æ¨¡å‹
- **âš¡ é«˜æ€§èƒ½**: æ™ºèƒ½ç¼“å­˜ã€è¿æ¥æ± ã€è¯·æ±‚ä¼˜åŒ–
- **ğŸ›¡ï¸ å¯é æ€§**: é‡è¯•æœºåˆ¶ã€é”™è¯¯å¤„ç†ã€é™çº§ç­–ç•¥
- **ğŸ“Š å¯è§‚æµ‹æ€§**: å®Œæ•´çš„ç›‘æ§ã€æ—¥å¿—ã€æ€§èƒ½æŒ‡æ ‡
- **ğŸ”§ æ˜“æ‰©å±•**: æ’ä»¶åŒ–æ¶æ„ï¼Œæ˜“äºæ·»åŠ æ–°çš„ AI æä¾›å•†
- **ğŸ’° æˆæœ¬ä¼˜åŒ–**: æ™ºèƒ½è·¯ç”±ã€ç¼“å­˜ç­–ç•¥ã€ä½¿ç”¨é‡ç›‘æ§

### 1.3 æŠ€æœ¯æ¶æ„

```typescript
// æ ¸å¿ƒæ¶æ„æ¦‚è§ˆ
interface AIArchitecture {
  // æä¾›å•†å±‚ï¼šAI æœåŠ¡æŠ½è±¡
  providers: {
    openai: OpenAIProvider
    anthropic: AnthropicProvider
    local: LocalModelProvider
    custom: CustomProvider[]
  }

  // ç®¡ç†å±‚ï¼šæœåŠ¡ç®¡ç†å’Œåè°ƒ
  management: {
    registry: AIRegistry
    config: ConfigManager
    cache: CacheManager
    monitor: MonitoringManager
  }

  // æœåŠ¡å±‚ï¼šç»Ÿä¸€ AI æœåŠ¡æ¥å£
  services: {
    completion: CompletionService
    chat: ChatService
    embedding: EmbeddingService
    image: ImageService
  }

  // å·¥å…·å±‚ï¼šè¾…åŠ©åŠŸèƒ½
  utilities: {
    tokenizer: TokenizerUtils
    validator: ValidationUtils
    formatter: ResponseFormatter
  }
}
```

### 1.4 èŒè´£è¾¹ç•Œ

| èŒè´£èŒƒå›´ | åŒ…å«åŠŸèƒ½ | ä¸åŒ…å«åŠŸèƒ½ |
|---------|---------|-----------|
| **AI æœåŠ¡æŠ½è±¡** | æä¾›å•†æ¥å£ã€ç»Ÿä¸€è°ƒç”¨ã€åè®®è½¬æ¢ | å…·ä½“ AI æ¨¡å‹è®­ç»ƒ |
| **é…ç½®ç®¡ç†** | å¤šæä¾›å•†é…ç½®ã€è®¤è¯ç®¡ç†ã€ç¯å¢ƒåˆ‡æ¢ | ä¸šåŠ¡é…ç½®é€»è¾‘ |
| **æ€§èƒ½ä¼˜åŒ–** | ç¼“å­˜ç­–ç•¥ã€è¿æ¥æ± ã€è¯·æ±‚ä¼˜åŒ– | ä¸šåŠ¡å±‚æ€§èƒ½ä¼˜åŒ– |
| **ç›‘æ§è§‚æµ‹** | ä½¿ç”¨é‡ç»Ÿè®¡ã€æ€§èƒ½ç›‘æ§ã€é”™è¯¯è¿½è¸ª | ä¸šåŠ¡æŒ‡æ ‡ç›‘æ§ |
| **æ‰©å±•æ”¯æŒ** | æ’ä»¶æ¥å£ã€è‡ªå®šä¹‰æä¾›å•† | ä¸šåŠ¡æ’ä»¶å¼€å‘ |

---

## 2. API è®¾è®¡

### 2.1 æ ¸å¿ƒæ¥å£è®¾è®¡

#### AI æä¾›å•†æŠ½è±¡æ¥å£

```typescript
// åŸºç¡€ AI æä¾›å•†æ¥å£
export abstract class AIProvider {
  abstract readonly name: string
  abstract readonly version: string
  abstract readonly capabilities: AICapabilities

  // ç”Ÿå‘½å‘¨æœŸç®¡ç†
  abstract initialize(config: AIConfig): Promise<void>
  abstract destroy(): Promise<void>
  abstract healthCheck(): Promise<HealthCheckResult>

  // æ–‡æœ¬ç”Ÿæˆ
  abstract complete(
    prompt: string,
    options?: CompletionOptions
  ): Promise<CompletionResponse>

  // å¯¹è¯ç”Ÿæˆ
  abstract chat(
    messages: ChatMessage[],
    options?: ChatOptions
  ): Promise<ChatResponse>

  // åµŒå…¥å‘é‡
  abstract embeddings(
    texts: string[],
    options?: EmbeddingOptions
  ): Promise<EmbeddingResponse>

  // æµå¼å“åº”
  abstract streamComplete(
    prompt: string,
    options?: StreamOptions
  ): AsyncIterable<CompletionChunk>

  abstract streamChat(
    messages: ChatMessage[],
    options?: StreamOptions
  ): AsyncIterable<ChatChunk>

  // å‡½æ•°è°ƒç”¨
  abstract callFunction(
    messages: ChatMessage[],
    functions: FunctionDefinition[],
    options?: FunctionCallOptions
  ): Promise<FunctionCallResponse>

  // å¤šæ¨¡æ€èƒ½åŠ›ï¼ˆå¯é€‰ï¼‰
  abstract generateImage?(
    prompt: string,
    options?: ImageGenerationOptions
  ): Promise<ImageResponse>

  abstract analyzeImage?(
    image: ImageInput,
    prompt: string,
    options?: ImageAnalysisOptions
  ): Promise<ImageAnalysisResponse>

  abstract processAudio?(
    audio: AudioInput,
    options?: AudioProcessingOptions
  ): Promise<AudioResponse>
}

// AI èƒ½åŠ›å®šä¹‰
export interface AICapabilities {
  // åŸºç¡€æ–‡æœ¬èƒ½åŠ›
  completion: boolean
  chat: boolean
  embeddings: boolean

  // é«˜çº§åŠŸèƒ½
  streaming: boolean
  functionCalling: boolean
  jsonMode: boolean
  systemPrompt: boolean

  // å¤šæ¨¡æ€èƒ½åŠ›
  imageGeneration: boolean
  imageAnalysis: boolean
  audioProcessing: boolean
  videoProcessing: boolean

  // æ¨¡å‹ä¿¡æ¯
  models: ModelInfo[]
  maxTokens: number
  supportedLanguages: string[]

  // æ€§èƒ½ç‰¹å¾
  latency: 'low' | 'medium' | 'high'
  throughput: 'low' | 'medium' | 'high'
  costEfficiency: 'low' | 'medium' | 'high'
}

// æ¨¡å‹ä¿¡æ¯
export interface ModelInfo {
  id: string
  name: string
  description: string
  contextLength: number
  inputTypes: ('text' | 'image' | 'audio' | 'video')[]
  outputTypes: ('text' | 'image' | 'audio' | 'video')[]
  pricing?: {
    input: number  // per 1K tokens
    output: number // per 1K tokens
    image?: number // per image
    audio?: number // per minute
  }
  limits?: {
    requestsPerMinute: number
    tokensPerMinute: number
    concurrentRequests: number
  }
}
```

### 2.2 ç»Ÿä¸€æœåŠ¡æ¥å£

#### AI æœåŠ¡ç®¡ç†å™¨

```typescript
// ä¸»è¦çš„ AI æœåŠ¡æ¥å£
export interface AIService {
  // æä¾›å•†ç®¡ç†
  registerProvider(provider: AIProvider): void
  unregisterProvider(name: string): void
  listProviders(): ProviderInfo[]
  getProvider(name: string): AIProvider | undefined

  // é…ç½®ç®¡ç†
  configure(config: AIServiceConfig): Promise<void>
  setDefaultProvider(name: string, config?: AIConfig): Promise<void>

  // æ–‡æœ¬ç”ŸæˆæœåŠ¡
  complete(
    prompt: string,
    options?: CompletionServiceOptions
  ): Promise<CompletionResponse>

  // å¯¹è¯æœåŠ¡
  chat(
    messages: ChatMessage[],
    options?: ChatServiceOptions
  ): Promise<ChatResponse>

  // æµå¼å¯¹è¯
  streamChat(
    messages: ChatMessage[],
    options?: StreamServiceOptions
  ): AsyncIterable<ChatChunk>

  // åµŒå…¥æœåŠ¡
  embeddings(
    texts: string[],
    options?: EmbeddingServiceOptions
  ): Promise<EmbeddingResponse>

  // å‡½æ•°è°ƒç”¨æœåŠ¡
  callFunction(
    messages: ChatMessage[],
    functions: FunctionDefinition[],
    options?: FunctionServiceOptions
  ): Promise<FunctionCallResponse>

  // å¤šæ¨¡æ€æœåŠ¡
  generateImage(
    prompt: string,
    options?: ImageGenerationServiceOptions
  ): Promise<ImageResponse>

  analyzeImage(
    image: ImageInput,
    prompt: string,
    options?: ImageAnalysisServiceOptions
  ): Promise<ImageAnalysisResponse>

  // å·¥å…·æ–¹æ³•
  estimateTokens(text: string, model?: string): number
  estimateCost(usage: TokenUsage, model?: string): number

  // ç›‘æ§å’Œç»Ÿè®¡
  getUsageStats(timeRange?: TimeRange): Promise<UsageStats>
  getPerformanceMetrics(): Promise<PerformanceMetrics>
}

// æœåŠ¡é…ç½®
export interface AIServiceConfig {
  // é»˜è®¤æä¾›å•†
  defaultProvider?: string

  // æä¾›å•†é…ç½®
  providers: Record<string, AIConfig>

  // ç¼“å­˜é…ç½®
  cache?: {
    enabled: boolean
    ttl: number
    maxSize: number
    strategy: 'memory' | 'redis' | 'hybrid'
  }

  // é‡è¯•é…ç½®
  retry?: {
    enabled: boolean
    maxAttempts: number
    backoffStrategy: 'linear' | 'exponential'
    baseDelay: number
  }

  // ç›‘æ§é…ç½®
  monitoring?: {
    enabled: boolean
    metricsInterval: number
    logLevel: 'debug' | 'info' | 'warn' | 'error'
  }

  // é™æµé…ç½®
  rateLimit?: {
    enabled: boolean
    requestsPerMinute: number
    tokensPerMinute: number
  }
}
```

### 2.3 è¯·æ±‚å’Œå“åº”ç±»å‹

#### é€šç”¨ç±»å‹å®šä¹‰

```typescript
// èŠå¤©æ¶ˆæ¯
export interface ChatMessage {
  role: 'system' | 'user' | 'assistant' | 'function'
  content: string
  name?: string
  functionCall?: FunctionCall
}

// å‡½æ•°è°ƒç”¨
export interface FunctionCall {
  name: string
  arguments: string
}

// å‡½æ•°å®šä¹‰
export interface FunctionDefinition {
  name: string
  description: string
  parameters: {
    type: 'object'
    properties: Record<string, any>
    required?: string[]
  }
}

// å®Œæˆå“åº”
export interface CompletionResponse {
  text: string
  usage: TokenUsage
  model: string
  finishReason: 'stop' | 'length' | 'content_filter' | 'function_call'
  metadata?: Record<string, any>
}

// èŠå¤©å“åº”
export interface ChatResponse {
  message: ChatMessage
  usage: TokenUsage
  model: string
  finishReason: 'stop' | 'length' | 'content_filter' | 'function_call'
  metadata?: Record<string, any>
}

// æµå¼å“åº”å—
export interface ChatChunk {
  content?: string
  role?: string
  finishReason?: string
  functionCall?: Partial<FunctionCall>
  metadata?: Record<string, any>
}

// åµŒå…¥å“åº”
export interface EmbeddingResponse {
  embeddings: number[][]
  usage: TokenUsage
  model: string
  metadata?: Record<string, any>
}

// Token ä½¿ç”¨æƒ…å†µ
export interface TokenUsage {
  promptTokens: number
  completionTokens: number
  totalTokens: number
}

// å¥åº·æ£€æŸ¥ç»“æœ
export interface HealthCheckResult {
  healthy: boolean
  latency?: number
  error?: string
  metadata?: Record<string, any>
}
```

### 2.4 é…ç½®æ¥å£

#### æä¾›å•†é…ç½®

```typescript
// åŸºç¡€é…ç½®æ¥å£
export interface AIConfig {
  apiKey?: string
  baseURL?: string
  timeout?: number
  retries?: number
  metadata?: Record<string, any>
}

// OpenAI é…ç½®
export interface OpenAIConfig extends AIConfig {
  organization?: string
  project?: string
  dangerouslyAllowBrowser?: boolean
}

// Anthropic é…ç½®
export interface AnthropicConfig extends AIConfig {
  version?: string
  maxRetries?: number
}

// æœ¬åœ°æ¨¡å‹é…ç½®
export interface LocalModelConfig extends AIConfig {
  modelPath: string
  device?: 'cpu' | 'gpu' | 'auto'
  threads?: number
  contextSize?: number
}

// è‡ªå®šä¹‰æä¾›å•†é…ç½®
export interface CustomProviderConfig extends AIConfig {
  endpoint: string
  headers?: Record<string, string>
  authentication?: {
    type: 'bearer' | 'api-key' | 'basic' | 'custom'
    credentials: Record<string, string>
  }
}
```

### 2.5 é”™è¯¯å¤„ç†æ¥å£

#### é”™è¯¯ç±»å‹å®šä¹‰

```typescript
// AI é”™è¯¯åŸºç±»
export class AIError extends Error {
  constructor(
    message: string,
    public code: string,
    public provider?: string,
    public details?: any
  ) {
    super(message)
    this.name = 'AIError'
  }
}

// å…·ä½“é”™è¯¯ç±»å‹
export class AIProviderError extends AIError {
  constructor(message: string, provider: string, details?: any) {
    super(message, 'PROVIDER_ERROR', provider, details)
  }
}

export class AIConfigurationError extends AIError {
  constructor(message: string, details?: any) {
    super(message, 'CONFIGURATION_ERROR', undefined, details)
  }
}

export class AIRateLimitError extends AIError {
  constructor(message: string, provider: string, retryAfter?: number) {
    super(message, 'RATE_LIMIT_ERROR', provider, { retryAfter })
  }
}

export class AIQuotaExceededError extends AIError {
  constructor(message: string, provider: string, details?: any) {
    super(message, 'QUOTA_EXCEEDED_ERROR', provider, details)
  }
}

export class AIValidationError extends AIError {
  constructor(message: string, field: string, value: any) {
    super(message, 'VALIDATION_ERROR', undefined, { field, value })
  }
}

// é”™è¯¯å¤„ç†å™¨æ¥å£
export interface ErrorHandler {
  handle(error: Error, context: ErrorContext): Promise<ErrorHandlingResult>
  canHandle(error: Error): boolean
  priority: number
}

export interface ErrorContext {
  provider: string
  operation: string
  input: any
  attempt: number
  timestamp: Date
}

export interface ErrorHandlingResult {
  action: 'retry' | 'fallback' | 'fail'
  delay?: number
  fallbackProvider?: string
  transformedError?: Error
}
```

---

## 3. å®ç°ç»†èŠ‚

### 3.1 AI æ³¨å†Œè¡¨å®ç°

#### æä¾›å•†æ³¨å†Œå’Œç®¡ç†

```typescript
export class AIRegistry {
  private providers: Map<string, typeof AIProvider> = new Map()
  private instances: Map<string, AIProvider> = new Map()
  private configs: Map<string, AIConfig> = new Map()
  private healthStatus: Map<string, HealthCheckResult> = new Map()

  constructor(private logger: Logger) {}

  // æ³¨å†Œæä¾›å•†ç±»
  registerProvider(ProviderClass: typeof AIProvider): void {
    const provider = new ProviderClass()
    this.providers.set(provider.name, ProviderClass)
    this.logger.info(`Registered AI provider: ${provider.name}`)
  }

  // åˆ›å»ºæä¾›å•†å®ä¾‹
  async createInstance(
    providerName: string,
    config: AIConfig
  ): Promise<AIProvider> {
    const ProviderClass = this.providers.get(providerName)
    if (!ProviderClass) {
      throw new AIProviderError(`Provider '${providerName}' not found`, providerName)
    }

    const configHash = this.hashConfig(config)
    const instanceKey = `${providerName}:${configHash}`

    // æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å®ä¾‹
    const existingInstance = this.instances.get(instanceKey)
    if (existingInstance) {
      return existingInstance
    }

    // åˆ›å»ºæ–°å®ä¾‹
    const instance = new ProviderClass()

    try {
      await instance.initialize(config)

      // å¥åº·æ£€æŸ¥
      const healthResult = await instance.healthCheck()
      this.healthStatus.set(instanceKey, healthResult)

      if (!healthResult.healthy) {
        throw new AIProviderError(
          `Provider '${providerName}' failed health check: ${healthResult.error}`,
          providerName,
          healthResult
        )
      }

      this.instances.set(instanceKey, instance)
      this.configs.set(instanceKey, config)

      this.logger.info(`Created AI provider instance: ${instanceKey}`)
      return instance

    } catch (error) {
      this.logger.error(`Failed to create AI provider instance: ${instanceKey}`, error)
      throw error
    }
  }

  // è·å–æä¾›å•†å®ä¾‹
  getInstance(providerName: string, configHash?: string): AIProvider | undefined {
    if (configHash) {
      return this.instances.get(`${providerName}:${configHash}`)
    }

    // è¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…çš„å¥åº·å®ä¾‹
    for (const [key, instance] of this.instances) {
      if (key.startsWith(`${providerName}:`)) {
        const health = this.healthStatus.get(key)
        if (health?.healthy) {
          return instance
        }
      }
    }

    return undefined
  }

  // é”€æ¯å®ä¾‹
  async destroyInstance(providerName: string, configHash?: string): Promise<void> {
    const pattern = configHash ? `${providerName}:${configHash}` : `${providerName}:`

    for (const [key, instance] of this.instances) {
      if (key.startsWith(pattern)) {
        try {
          await instance.destroy()
          this.instances.delete(key)
          this.configs.delete(key)
          this.healthStatus.delete(key)
          this.logger.info(`Destroyed AI provider instance: ${key}`)
        } catch (error) {
          this.logger.error(`Failed to destroy AI provider instance: ${key}`, error)
        }
      }
    }
  }

  // åˆ—å‡ºæ‰€æœ‰æä¾›å•†
  listProviders(): ProviderInfo[] {
    return Array.from(this.providers.entries()).map(([name, ProviderClass]) => {
      const provider = new ProviderClass()
      const instances = this.getProviderInstances(name)

      return {
        name,
        version: provider.version,
        capabilities: provider.capabilities,
        instanceCount: instances.length,
        healthyInstances: instances.filter(i => i.healthy).length
      }
    })
  }

  // è·å–æä¾›å•†èƒ½åŠ›
  getCapabilities(providerName: string): AICapabilities | undefined {
    const ProviderClass = this.providers.get(providerName)
    if (!ProviderClass) return undefined

    const provider = new ProviderClass()
    return provider.capabilities
  }

  // å¥åº·æ£€æŸ¥
  async performHealthChecks(): Promise<void> {
    const checks = Array.from(this.instances.entries()).map(async ([key, instance]) => {
      try {
        const result = await instance.healthCheck()
        this.healthStatus.set(key, result)

        if (!result.healthy) {
          this.logger.warn(`Provider instance unhealthy: ${key}`, result)
        }
      } catch (error) {
        this.healthStatus.set(key, {
          healthy: false,
          error: error.message
        })
        this.logger.error(`Health check failed for: ${key}`, error)
      }
    })

    await Promise.all(checks)
  }

  private hashConfig(config: AIConfig): string {
    // åˆ›å»ºé…ç½®çš„å“ˆå¸Œå€¼ï¼Œæ’é™¤æ•æ„Ÿä¿¡æ¯
    const sanitized = { ...config }
    delete sanitized.apiKey
    return crypto.createHash('md5').update(JSON.stringify(sanitized)).digest('hex')
  }

  private getProviderInstances(providerName: string): InstanceInfo[] {
    const instances: InstanceInfo[] = []

    for (const [key, instance] of this.instances) {
      if (key.startsWith(`${providerName}:`)) {
        const health = this.healthStatus.get(key)
        instances.push({
          key,
          healthy: health?.healthy || false,
          latency: health?.latency,
          error: health?.error
        })
      }
    }

    return instances
  }
}

interface ProviderInfo {
  name: string
  version: string
  capabilities: AICapabilities
  instanceCount: number
  healthyInstances: number
}

interface InstanceInfo {
  key: string
  healthy: boolean
  latency?: number
  error?: string
}
```

### 3.2 OpenAI æä¾›å•†å®ç°

#### OpenAI æä¾›å•†ç±»

```typescript
export class OpenAIProvider extends AIProvider {
  readonly name = 'openai'
  readonly version = '1.0.0'
  readonly capabilities: AICapabilities = {
    completion: true,
    chat: true,
    embeddings: true,
    streaming: true,
    functionCalling: true,
    jsonMode: true,
    systemPrompt: true,
    imageGeneration: true,
    imageAnalysis: true,
    audioProcessing: true,
    videoProcessing: false,
    models: [
      {
        id: 'gpt-4-turbo',
        name: 'GPT-4 Turbo',
        description: 'Most capable model with vision',
        contextLength: 128000,
        inputTypes: ['text', 'image'],
        outputTypes: ['text'],
        pricing: { input: 0.01, output: 0.03 },
        limits: {
          requestsPerMinute: 500,
          tokensPerMinute: 150000,
          concurrentRequests: 10
        }
      },
      {
        id: 'gpt-3.5-turbo',
        name: 'GPT-3.5 Turbo',
        description: 'Fast and efficient model',
        contextLength: 16385,
        inputTypes: ['text'],
        outputTypes: ['text'],
        pricing: { input: 0.0005, output: 0.0015 },
        limits: {
          requestsPerMinute: 3500,
          tokensPerMinute: 90000,
          concurrentRequests: 20
        }
      },
      {
        id: 'text-embedding-3-large',
        name: 'Text Embedding 3 Large',
        description: 'Most capable embedding model',
        contextLength: 8191,
        inputTypes: ['text'],
        outputTypes: ['embedding'],
        pricing: { input: 0.00013, output: 0 }
      }
    ],
    maxTokens: 128000,
    supportedLanguages: ['en', 'zh', 'ja', 'ko', 'es', 'fr', 'de', 'it', 'pt', 'ru'],
    latency: 'medium',
    throughput: 'high',
    costEfficiency: 'medium'
  }

  private client?: OpenAI
  private config?: OpenAIConfig
  private rateLimiter?: RateLimiter

  async initialize(config: OpenAIConfig): Promise<void> {
    this.config = config
    this.client = new OpenAI({
      apiKey: config.apiKey,
      baseURL: config.baseURL,
      organization: config.organization,
      project: config.project,
      timeout: config.timeout || 60000,
      maxRetries: config.retries || 3,
    })

    // åˆå§‹åŒ–é™æµå™¨
    this.rateLimiter = new RateLimiter({
      requestsPerMinute: 500,
      tokensPerMinute: 150000
    })

    // éªŒè¯è¿æ¥
    const healthResult = await this.healthCheck()
    if (!healthResult.healthy) {
      throw new AIProviderError(
        `OpenAI provider initialization failed: ${healthResult.error}`,
        this.name,
        healthResult
      )
    }
  }

  async destroy(): Promise<void> {
    this.client = undefined
    this.config = undefined
    this.rateLimiter = undefined
  }

  async healthCheck(): Promise<HealthCheckResult> {
    const startTime = Date.now()

    try {
      if (!this.client) {
        return {
          healthy: false,
          error: 'Client not initialized'
        }
      }

      await this.client.models.list()
      const latency = Date.now() - startTime

      return {
        healthy: true,
        latency,
        metadata: {
          provider: this.name,
          timestamp: new Date().toISOString()
        }
      }
    } catch (error) {
      return {
        healthy: false,
        latency: Date.now() - startTime,
        error: error.message,
        metadata: {
          provider: this.name,
          timestamp: new Date().toISOString()
        }
      }
    }
  }

  async complete(
    prompt: string,
    options: OpenAICompletionOptions = {}
  ): Promise<CompletionResponse> {
    if (!this.client) {
      throw new AIProviderError('OpenAI provider not initialized', this.name)
    }

    // é™æµæ£€æŸ¥
    await this.rateLimiter?.checkLimit('completion')

    try {
      const response = await this.client.completions.create({
        model: options.model || 'gpt-3.5-turbo-instruct',
        prompt,
        max_tokens: options.maxTokens || 1000,
        temperature: options.temperature ?? 0.7,
        top_p: options.topP,
        frequency_penalty: options.frequencyPenalty,
        presence_penalty: options.presencePenalty,
        stop: options.stop,
        user: options.user,
      })

      return {
        text: response.choices[0]?.text || '',
        usage: {
          promptTokens: response.usage?.prompt_tokens || 0,
          completionTokens: response.usage?.completion_tokens || 0,
          totalTokens: response.usage?.total_tokens || 0,
        },
        model: response.model,
        finishReason: this.mapFinishReason(response.choices[0]?.finish_reason),
        metadata: {
          id: response.id,
          created: response.created,
          provider: this.name
        }
      }
    } catch (error) {
      throw this.handleError(error, 'completion', { prompt, options })
    }
  }

  async chat(
    messages: ChatMessage[],
    options: OpenAIChatOptions = {}
  ): Promise<ChatResponse> {
    if (!this.client) {
      throw new AIProviderError('OpenAI provider not initialized', this.name)
    }

    // é™æµæ£€æŸ¥
    await this.rateLimiter?.checkLimit('chat')

    try {
      const response = await this.client.chat.completions.create({
        model: options.model || 'gpt-3.5-turbo',
        messages: this.formatMessages(messages),
        max_tokens: options.maxTokens,
        temperature: options.temperature ?? 0.7,
        top_p: options.topP,
        frequency_penalty: options.frequencyPenalty,
        presence_penalty: options.presencePenalty,
        stop: options.stop,
        functions: options.functions,
        function_call: options.functionCall,
        response_format: options.responseFormat,
        user: options.user,
      })

      const choice = response.choices[0]
      return {
        message: {
          role: choice?.message?.role || 'assistant',
          content: choice?.message?.content || '',
          functionCall: choice?.message?.function_call ? {
            name: choice.message.function_call.name,
            arguments: choice.message.function_call.arguments
          } : undefined,
        },
        usage: {
          promptTokens: response.usage?.prompt_tokens || 0,
          completionTokens: response.usage?.completion_tokens || 0,
          totalTokens: response.usage?.total_tokens || 0,
        },
        model: response.model,
        finishReason: this.mapFinishReason(choice?.finish_reason),
        metadata: {
          id: response.id,
          created: response.created,
          provider: this.name
        }
      }
    } catch (error) {
      throw this.handleError(error, 'chat', { messages, options })
    }
  }

  async *streamChat(
    messages: ChatMessage[],
    options: OpenAIStreamOptions = {}
  ): AsyncIterable<ChatChunk> {
    if (!this.client) {
      throw new AIProviderError('OpenAI provider not initialized', this.name)
    }

    // é™æµæ£€æŸ¥
    await this.rateLimiter?.checkLimit('stream')

    try {
      const stream = await this.client.chat.completions.create({
        model: options.model || 'gpt-3.5-turbo',
        messages: this.formatMessages(messages),
        stream: true,
        max_tokens: options.maxTokens,
        temperature: options.temperature ?? 0.7,
        top_p: options.topP,
        frequency_penalty: options.frequencyPenalty,
        presence_penalty: options.presencePenalty,
        stop: options.stop,
        user: options.user,
      })

      for await (const chunk of stream) {
        const delta = chunk.choices[0]?.delta
        if (delta) {
          yield {
            content: delta.content || undefined,
            role: delta.role || undefined,
            finishReason: chunk.choices[0]?.finish_reason || undefined,
            functionCall: delta.function_call ? {
              name: delta.function_call.name,
              arguments: delta.function_call.arguments
            } : undefined,
            metadata: {
              id: chunk.id,
              created: chunk.created,
              provider: this.name
            }
          }
        }
      }
    } catch (error) {
      throw this.handleError(error, 'streamChat', { messages, options })
    }
  }

  async embeddings(
    texts: string[],
    options: OpenAIEmbeddingOptions = {}
  ): Promise<EmbeddingResponse> {
    if (!this.client) {
      throw new AIProviderError('OpenAI provider not initialized', this.name)
    }

    try {
      const response = await this.client.embeddings.create({
        model: options.model || 'text-embedding-3-small',
        input: texts,
        encoding_format: options.encodingFormat || 'float',
        dimensions: options.dimensions,
        user: options.user,
      })

      return {
        embeddings: response.data.map(item => item.embedding),
        usage: {
          promptTokens: response.usage.prompt_tokens,
          completionTokens: 0,
          totalTokens: response.usage.total_tokens,
        },
        model: response.model,
        metadata: {
          provider: this.name,
          dimensions: response.data[0]?.embedding.length
        }
      }
    } catch (error) {
      throw this.handleError(error, 'embeddings', { texts, options })
    }
  }

  private formatMessages(messages: ChatMessage[]): any[] {
    return messages.map(msg => ({
      role: msg.role,
      content: msg.content,
      name: msg.name,
      function_call: msg.functionCall ? {
        name: msg.functionCall.name,
        arguments: msg.functionCall.arguments
      } : undefined
    }))
  }

  private mapFinishReason(reason?: string): string {
    const reasonMap: Record<string, string> = {
      'stop': 'stop',
      'length': 'length',
      'content_filter': 'content_filter',
      'function_call': 'function_call',
      'tool_calls': 'function_call'
    }
    return reasonMap[reason || ''] || 'unknown'
  }

  private handleError(error: any, operation: string, context: any): AIError {
    if (error.status === 429) {
      return new AIRateLimitError(
        `OpenAI rate limit exceeded for ${operation}`,
        this.name,
        error.headers?.['retry-after']
      )
    }

    if (error.status === 402) {
      return new AIQuotaExceededError(
        `OpenAI quota exceeded for ${operation}`,
        this.name,
        { usage: context }
      )
    }

    return new AIProviderError(
      `OpenAI ${operation} failed: ${error.message}`,
      this.name,
      { error, context }
    )
  }
}
```

### 3.3 ç¼“å­˜ç®¡ç†å™¨å®ç°

#### æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ

```typescript
export class CacheManager {
  private cache: Map<string, CacheEntry> = new Map()
  private accessOrder: string[] = []
  private stats = {
    hits: 0,
    misses: 0,
    evictions: 0,
    totalRequests: 0
  }

  constructor(private config: CacheConfig, private logger: Logger) {}

  async get<T>(key: string): Promise<T | null> {
    this.stats.totalRequests++

    const entry = this.cache.get(key)

    if (!entry || this.isExpired(entry)) {
      this.stats.misses++
      if (entry) {
        this.cache.delete(key)
        this.removeFromAccessOrder(key)
      }
      return null
    }

    this.stats.hits++
    this.updateAccessOrder(key)

    return entry.value as T
  }

  async set<T>(key: string, value: T, ttl?: number): Promise<void> {
    const entry: CacheEntry = {
      value,
      createdAt: Date.now(),
      ttl: ttl || this.config.defaultTTL,
      size: this.estimateSize(value)
    }

    if (this.cache.size >= this.config.maxSize) {
      await this.evictEntries()
    }

    this.cache.set(key, entry)
    this.updateAccessOrder(key)
  }

  private isExpired(entry: CacheEntry): boolean {
    return Date.now() - entry.createdAt > entry.ttl
  }

  private async evictEntries(): Promise<void> {
    const evictionCount = Math.max(1, Math.floor(this.config.maxSize * 0.1))

    for (let i = 0; i < evictionCount && this.accessOrder.length > 0; i++) {
      const keyToEvict = this.accessOrder.shift()!
      this.cache.delete(keyToEvict)
      this.stats.evictions++
    }
  }

  private updateAccessOrder(key: string): void {
    this.removeFromAccessOrder(key)
    this.accessOrder.push(key)
  }

  private removeFromAccessOrder(key: string): void {
    const index = this.accessOrder.indexOf(key)
    if (index > -1) {
      this.accessOrder.splice(index, 1)
    }
  }

  private estimateSize(value: any): number {
    try {
      return JSON.stringify(value).length * 2
    } catch {
      return 1000
    }
  }
}
```

### 3.4 AI æœåŠ¡ç®¡ç†å™¨å®ç°

```typescript
export class AIServiceManager implements AIService {
  private registry: AIRegistry
  private cacheManager: CacheManager
  private config: AIServiceConfig
  private defaultProvider?: string

  constructor(config: AIServiceConfig, logger: Logger) {
    this.config = config
    this.registry = new AIRegistry(logger)
    this.cacheManager = new CacheManager(config.cache || {
      defaultTTL: 5 * 60 * 1000,
      maxSize: 1000
    }, logger)

    this.registerBuiltinProviders()
  }

  async configure(config: AIServiceConfig): Promise<void> {
    this.config = { ...this.config, ...config }

    for (const [providerName, providerConfig] of Object.entries(config.providers)) {
      try {
        await this.registry.createInstance(providerName, providerConfig)
      } catch (error) {
        console.error(`Failed to configure provider ${providerName}:`, error)
      }
    }

    if (config.defaultProvider) {
      this.defaultProvider = config.defaultProvider
    }
  }

  registerProvider(provider: AIProvider): void {
    this.registry.registerProvider(provider.constructor as typeof AIProvider)
  }

  async complete(
    prompt: string,
    options: CompletionServiceOptions = {}
  ): Promise<CompletionResponse> {
    const provider = this.getProviderForRequest(options.provider)
    const cacheKey = this.generateCacheKey('complete', { prompt, options })

    if (this.config.cache?.enabled) {
      const cached = await this.cacheManager.get<CompletionResponse>(cacheKey)
      if (cached) return cached
    }

    const response = await provider.complete(prompt, options)

    if (this.config.cache?.enabled) {
      await this.cacheManager.set(cacheKey, response, this.config.cache.ttl)
    }

    return response
  }

  private getProviderForRequest(providerName?: string): AIProvider {
    const targetProvider = providerName || this.defaultProvider

    if (!targetProvider) {
      throw new AIConfigurationError('No provider specified and no default provider set')
    }

    const provider = this.registry.getInstance(targetProvider)
    if (!provider) {
      throw new AIProviderError(`Provider '${targetProvider}' not found`, targetProvider)
    }

    return provider
  }

  private generateCacheKey(operation: string, params: any): string {
    const sanitized = { ...params }
    delete sanitized.apiKey
    delete sanitized.user

    const hash = crypto.createHash('md5').update(JSON.stringify(sanitized)).digest('hex')
    return `${operation}:${hash}`
  }

  private registerBuiltinProviders(): void {
    this.registry.registerProvider(OpenAIProvider)
    this.registry.registerProvider(AnthropicProvider)
  }
}
```

---

## 4. é›†æˆæ¥å£

### 4.1 ä¸ @linch-kit/core é›†æˆ

#### æ’ä»¶ç³»ç»Ÿé›†æˆ

```typescript
import { Plugin, PluginManager } from '@linch-kit/core'

export class AIPlugin implements Plugin {
  id = 'ai'
  name = 'AI Integration Plugin'
  version = '1.0.0'
  description = 'Provides AI service integration with multiple providers'

  private aiService?: AIServiceManager

  async setup(context: PluginContext): Promise<void> {
    // æ’ä»¶è®¾ç½®é˜¶æ®µ
    console.log('AI plugin setup')
  }

  async activate(context: PluginContext): Promise<void> {
    // æ³¨å†Œ AI ç›¸å…³é’©å­
    context.hooks.register('ai:before-request', this.beforeRequest)
    context.hooks.register('ai:after-request', this.afterRequest)
    context.hooks.register('ai:error', this.onError)

    // æ³¨å†Œ AI æœåŠ¡
    const config = context.getConfig('ai') as AIServiceConfig
    this.aiService = new AIServiceManager(config, context.logger)

    // å°† AI æœåŠ¡æ³¨å†Œåˆ°æ’ä»¶ä¸Šä¸‹æ–‡
    context.registerService('ai', this.aiService)
  }

  async deactivate(context: PluginContext): Promise<void> {
    // æ¸…ç†é’©å­æ³¨å†Œ
    context.hooks.unregister('ai:before-request', this.beforeRequest)
    context.hooks.unregister('ai:after-request', this.afterRequest)
    context.hooks.unregister('ai:error', this.onError)
  }

  async teardown(context: PluginContext): Promise<void> {
    if (this.aiService) {
      const providers = this.aiService.listProviders()
      for (const provider of providers) {
        this.aiService.unregisterProvider(provider.name)
      }
    }
  }

  private async beforeRequest(context: any): Promise<void> {
    console.log('AI request starting:', context)
  }

  private async afterRequest(context: any): Promise<void> {
    console.log('AI request completed:', context)
  }

  private async onError(context: any): Promise<void> {
    console.error('AI request error:', context)
  }
}
```

### 4.2 ä¸å…¶ä»–åŒ…çš„é›†æˆ

#### ä¸ @linch-kit/schema é›†æˆ

```typescript
import { EntitySchema } from '@linch-kit/schema'

export class AISchemaIntegration {
  constructor(private aiService: AIService) {}

  async generateSchemaFromDescription(
    description: string,
    options: SchemaGenerationOptions = {}
  ): Promise<EntitySchema<any>> {
    const prompt = this.buildSchemaPrompt(description, options)

    const response = await this.aiService.chat([
      {
        role: 'system',
        content: 'You are a schema generation expert. Generate TypeScript schemas based on descriptions.'
      },
      {
        role: 'user',
        content: prompt
      }
    ], {
      provider: options.provider,
      model: options.model || 'gpt-4',
      responseFormat: { type: 'json_object' }
    })

    return this.parseSchemaResponse(response.message.content)
  }

  async validateSchemaWithAI(
    schema: EntitySchema<any>,
    context: ValidationContext
  ): Promise<ValidationResult> {
    const prompt = this.buildValidationPrompt(schema, context)

    const response = await this.aiService.complete(prompt, {
      provider: context.provider,
      maxTokens: 1000
    })

    return this.parseValidationResponse(response.text)
  }

  private buildSchemaPrompt(description: string, options: SchemaGenerationOptions): string {
    return `
Generate a TypeScript entity schema for: ${description}

Requirements:
- Include field types, validation rules, and relationships
- Follow best practices for data modeling
- Include appropriate indexes and constraints
- Format as JSON schema

Additional context: ${options.context || 'None'}
    `.trim()
  }
}
```

#### ä¸ @linch-kit/crud é›†æˆ

```typescript
import { CRUDManager } from '@linch-kit/crud'

export class AICRUDIntegration {
  constructor(private aiService: AIService) {}

  async generateCRUDOperations(
    entityName: string,
    schema: EntitySchema<any>,
    requirements: CRUDRequirements
  ): Promise<CRUDOperations> {
    const prompt = this.buildCRUDPrompt(entityName, schema, requirements)

    const response = await this.aiService.chat([
      {
        role: 'system',
        content: 'You are a CRUD operation generator. Create efficient database operations.'
      },
      {
        role: 'user',
        content: prompt
      }
    ], {
      model: 'gpt-4',
      responseFormat: { type: 'json_object' }
    })

    return this.parseCRUDResponse(response.message.content)
  }

  async optimizeQuery(
    query: QueryInput<any>,
    performance: PerformanceMetrics
  ): Promise<OptimizedQuery> {
    const prompt = this.buildQueryOptimizationPrompt(query, performance)

    const response = await this.aiService.complete(prompt, {
      maxTokens: 1500
    })

    return this.parseOptimizationResponse(response.text)
  }
}
```

---

## 5. æœ€ä½³å®è·µ

### 5.1 æä¾›å•†é€‰æ‹©æœ€ä½³å®è·µ

#### 1. æ ¹æ®ç”¨ä¾‹é€‰æ‹©æä¾›å•†

```typescript
// âœ… æ¨èï¼šæ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©æä¾›å•†
const getOptimalProvider = (task: AITask): ProviderConfig => {
  switch (task.type) {
    case 'code-generation':
      return {
        provider: 'openai',
        model: 'gpt-4',
        temperature: 0.1 // ä½æ¸©åº¦ç¡®ä¿ä»£ç å‡†ç¡®æ€§
      }

    case 'creative-writing':
      return {
        provider: 'anthropic',
        model: 'claude-3-sonnet',
        temperature: 0.8 // é«˜æ¸©åº¦å¢åŠ åˆ›é€ æ€§
      }

    case 'data-analysis':
      return {
        provider: 'openai',
        model: 'gpt-4-turbo',
        temperature: 0.2 // ä¸­ä½æ¸©åº¦å¹³è¡¡å‡†ç¡®æ€§å’Œçµæ´»æ€§
      }

    case 'embedding':
      return {
        provider: 'openai',
        model: 'text-embedding-3-large'
      }

    default:
      return {
        provider: 'openai',
        model: 'gpt-3.5-turbo',
        temperature: 0.7
      }
  }
}

// ä½¿ç”¨ç¤ºä¾‹
const task: AITask = { type: 'code-generation', context: 'TypeScript API' }
const config = getOptimalProvider(task)
const response = await aiService.complete(prompt, config)
```

#### 2. å®ç°æ™ºèƒ½é™çº§ç­–ç•¥

```typescript
// âœ… æ¨èï¼šå®ç°æä¾›å•†é™çº§
export class FallbackStrategy {
  private providers = [
    { name: 'openai', priority: 1, cost: 'medium' },
    { name: 'anthropic', priority: 2, cost: 'high' },
    { name: 'local', priority: 3, cost: 'low' }
  ]

  async executeWithFallback<T>(
    operation: (provider: AIProvider) => Promise<T>,
    options: FallbackOptions = {}
  ): Promise<T> {
    const sortedProviders = this.providers
      .filter(p => !options.excludeProviders?.includes(p.name))
      .sort((a, b) => a.priority - b.priority)

    let lastError: Error | undefined

    for (const providerConfig of sortedProviders) {
      try {
        const provider = await this.aiService.getProvider(providerConfig.name)
        const result = await operation(provider)

        // è®°å½•æˆåŠŸä½¿ç”¨çš„æä¾›å•†
        this.recordProviderSuccess(providerConfig.name)
        return result

      } catch (error) {
        lastError = error
        this.recordProviderFailure(providerConfig.name, error)

        // å¦‚æœæ˜¯é…é¢é”™è¯¯ï¼Œè·³è¿‡è¯¥æä¾›å•†
        if (error instanceof AIQuotaExceededError) {
          continue
        }

        // å¦‚æœæ˜¯æœ€åä¸€ä¸ªæä¾›å•†ï¼ŒæŠ›å‡ºé”™è¯¯
        if (providerConfig === sortedProviders[sortedProviders.length - 1]) {
          throw error
        }
      }
    }

    throw lastError || new Error('All providers failed')
  }
}
```

### 5.2 æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ

#### 1. æ™ºèƒ½ç¼“å­˜ç­–ç•¥

```typescript
// âœ… æ¨èï¼šåŸºäºå†…å®¹å’Œä¸Šä¸‹æ–‡çš„ç¼“å­˜ç­–ç•¥
export class SmartCacheStrategy {
  generateCacheKey(
    operation: string,
    input: any,
    options: any
  ): string {
    // æ ‡å‡†åŒ–è¾“å…¥ä»¥æé«˜ç¼“å­˜å‘½ä¸­ç‡
    const normalizedInput = this.normalizeInput(input)
    const relevantOptions = this.extractRelevantOptions(options)

    const cacheData = {
      operation,
      input: normalizedInput,
      options: relevantOptions
    }

    return crypto.createHash('sha256')
      .update(JSON.stringify(cacheData))
      .digest('hex')
  }

  private normalizeInput(input: any): any {
    if (typeof input === 'string') {
      // ç§»é™¤å¤šä½™ç©ºæ ¼ï¼Œç»Ÿä¸€æ¢è¡Œç¬¦
      return input.trim().replace(/\s+/g, ' ').replace(/\r\n/g, '\n')
    }

    if (Array.isArray(input)) {
      return input.map(item => this.normalizeInput(item))
    }

    if (typeof input === 'object' && input !== null) {
      const normalized: any = {}
      for (const [key, value] of Object.entries(input)) {
        normalized[key] = this.normalizeInput(value)
      }
      return normalized
    }

    return input
  }

  private extractRelevantOptions(options: any): any {
    // åªç¼“å­˜å½±å“ç»“æœçš„é€‰é¡¹
    const relevantKeys = [
      'model', 'temperature', 'maxTokens', 'topP',
      'frequencyPenalty', 'presencePenalty', 'stop'
    ]

    const relevant: any = {}
    for (const key of relevantKeys) {
      if (options[key] !== undefined) {
        relevant[key] = options[key]
      }
    }

    return relevant
  }
}
```

#### 2. è¯·æ±‚æ‰¹å¤„ç†

```typescript
// âœ… æ¨èï¼šæ‰¹é‡å¤„ç†ç›¸ä¼¼è¯·æ±‚
export class BatchProcessor {
  private pendingRequests: Map<string, PendingRequest[]> = new Map()
  private batchTimeout = 100 // 100ms

  async batchProcess<T>(
    key: string,
    request: () => Promise<T>
  ): Promise<T> {
    return new Promise((resolve, reject) => {
      const pending: PendingRequest = { resolve, reject, request }

      if (!this.pendingRequests.has(key)) {
        this.pendingRequests.set(key, [])

        // è®¾ç½®æ‰¹å¤„ç†å®šæ—¶å™¨
        setTimeout(() => {
          this.processBatch(key)
        }, this.batchTimeout)
      }

      this.pendingRequests.get(key)!.push(pending)
    })
  }

  private async processBatch(key: string): Promise<void> {
    const requests = this.pendingRequests.get(key) || []
    this.pendingRequests.delete(key)

    if (requests.length === 0) return

    try {
      // æ‰§è¡Œç¬¬ä¸€ä¸ªè¯·æ±‚ï¼Œå…¶ä»–è¯·æ±‚å…±äº«ç»“æœ
      const result = await requests[0].request()

      // æ‰€æœ‰è¯·æ±‚è¿”å›ç›¸åŒç»“æœ
      requests.forEach(req => req.resolve(result))

    } catch (error) {
      // æ‰€æœ‰è¯·æ±‚è¿”å›ç›¸åŒé”™è¯¯
      requests.forEach(req => req.reject(error))
    }
  }
}
```

### 5.3 é”™è¯¯å¤„ç†æœ€ä½³å®è·µ

#### 1. åˆ†å±‚é”™è¯¯å¤„ç†

```typescript
// âœ… æ¨èï¼šåˆ†å±‚é”™è¯¯å¤„ç†ç­–ç•¥
export class ErrorHandlingStrategy {
  private errorHandlers: ErrorHandler[] = [
    new RateLimitErrorHandler(),
    new QuotaErrorHandler(),
    new NetworkErrorHandler(),
    new ValidationErrorHandler(),
    new GenericErrorHandler()
  ]

  async handleError(
    error: Error,
    context: ErrorContext
  ): Promise<ErrorHandlingResult> {
    // æŒ‰ä¼˜å…ˆçº§å¤„ç†é”™è¯¯
    for (const handler of this.errorHandlers) {
      if (handler.canHandle(error)) {
        const result = await handler.handle(error, context)

        // è®°å½•é”™è¯¯å¤„ç†ç»“æœ
        this.logErrorHandling(error, handler, result)

        return result
      }
    }

    // é»˜è®¤å¤„ç†
    return { action: 'fail', transformedError: error }
  }
}

class RateLimitErrorHandler implements ErrorHandler {
  canHandle(error: Error): boolean {
    return error instanceof AIRateLimitError
  }

  async handle(error: AIRateLimitError, context: ErrorContext): Promise<ErrorHandlingResult> {
    const retryAfter = error.details?.retryAfter || 60

    if (context.attempt < 3) {
      return {
        action: 'retry',
        delay: retryAfter * 1000
      }
    }

    // å°è¯•é™çº§åˆ°å…¶ä»–æä¾›å•†
    const fallbackProvider = this.selectFallbackProvider(context.provider)
    if (fallbackProvider) {
      return {
        action: 'fallback',
        fallbackProvider
      }
    }

    return { action: 'fail', transformedError: error }
  }

  priority = 1
}
```

---

## 6. æ€§èƒ½è€ƒé‡

### 6.1 æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | å½“å‰å€¼ | ä¼˜åŒ–ç­–ç•¥ |
|------|--------|--------|----------|
| **API å“åº”æ—¶é—´** | < 5ç§’ | 3.2ç§’ | è¿æ¥æ± ã€ç¼“å­˜ä¼˜åŒ– |
| **æµå¼é¦–å­—èŠ‚** | < 100ms | 80ms | è¿æ¥å¤ç”¨ã€é¢„çƒ­ |
| **ç¼“å­˜å‘½ä¸­ç‡** | > 60% | 65% | æ™ºèƒ½ç¼“å­˜ç­–ç•¥ |
| **å¹¶å‘å¤„ç†** | 100+ | 150 | è¿æ¥æ± ã€é˜Ÿåˆ—ç®¡ç† |

### 6.2 èµ„æºç®¡ç†

#### å†…å­˜ä¼˜åŒ–

```typescript
export class MemoryManager {
  private memoryThreshold = 100 * 1024 * 1024 // 100MB
  private gcInterval = 5 * 60 * 1000 // 5åˆ†é’Ÿ

  constructor() {
    setInterval(() => this.performGC(), this.gcInterval)
  }

  private performGC(): void {
    const usage = process.memoryUsage()

    if (usage.heapUsed > this.memoryThreshold) {
      // æ¸…ç†è¿‡æœŸç¼“å­˜
      this.cacheManager.cleanup()

      // å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰
      if (global.gc) {
        global.gc()
      }

      console.log(`Memory GC performed. Usage: ${Math.round(usage.heapUsed / 1024 / 1024)}MB`)
    }
  }
}
```

---

## 7. æµ‹è¯•ç­–ç•¥

### 7.1 å•å…ƒæµ‹è¯•

```typescript
describe('AIServiceManager', () => {
  let aiService: AIServiceManager
  let mockProvider: jest.Mocked<AIProvider>

  beforeEach(() => {
    mockProvider = createMockProvider()
    aiService = new AIServiceManager(testConfig, mockLogger)
    aiService.registerProvider(mockProvider)
  })

  it('should complete text successfully', async () => {
    const prompt = 'Hello, world!'
    const expectedResponse = {
      text: 'Hello! How can I help you?',
      usage: { promptTokens: 3, completionTokens: 7, totalTokens: 10 },
      model: 'test-model',
      finishReason: 'stop'
    }

    mockProvider.complete.mockResolvedValue(expectedResponse)

    const result = await aiService.complete(prompt)

    expect(result).toEqual(expectedResponse)
    expect(mockProvider.complete).toHaveBeenCalledWith(prompt, {})
  })
})
```

---

## 8. AI é›†æˆæ”¯æŒ

### 8.1 è‡ªæˆ‘ä¼˜åŒ–èƒ½åŠ›

```typescript
export class SelfOptimizingAI {
  async optimizeProviderSelection(
    usage: UsageStats,
    performance: PerformanceMetrics
  ): Promise<ProviderOptimization> {
    const analysis = await this.aiService.chat([
      {
        role: 'system',
        content: 'You are an AI system optimizer. Analyze usage patterns and suggest improvements.'
      },
      {
        role: 'user',
        content: `Analyze this usage data and suggest optimal provider configurations: ${JSON.stringify({ usage, performance })}`
      }
    ])

    return this.parseOptimizationSuggestions(analysis.message.content)
  }
}
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

### ç›¸å…³æ–‡æ¡£
- [@linch-kit/core åŒ…æ–‡æ¡£](./core.md)
- [LinchKit æ¶æ„æ¦‚è§ˆ](../system-architecture.md)

### å¤–éƒ¨ä¾èµ–
- [OpenAI SDK](https://github.com/openai/openai-node)
- [Anthropic SDK](https://github.com/anthropics/anthropic-sdk-typescript)
- [Node.js](https://nodejs.org/)
- [TypeScript](https://www.typescriptlang.org/)

---

**æœ€åæ›´æ–°**: 2025-06-23
**æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0
**ç»´æŠ¤è€…**: LinchKit å¼€å‘å›¢é˜Ÿ

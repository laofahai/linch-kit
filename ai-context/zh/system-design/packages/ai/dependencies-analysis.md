# @linch-kit/ai ç¬¬ä¸‰æ–¹åº“ä¾èµ–åˆ†æ

> **åŒ…çŠ¶æ€**: å‡†å¤‡å¼€å‘ | **ä¼˜å…ˆçº§**: P2 | **ä¾èµ–ä¼˜åŒ–**: 93%è‡ªå»ºä»£ç å‡å°‘

## ğŸ¯ æ ¸å¿ƒç¬¬ä¸‰æ–¹åº“ç­–ç•¥

### 1. AI æœåŠ¡å®¢æˆ·ç«¯ (100%ç¬¬ä¸‰æ–¹)
- **openai**: OpenAIå®˜æ–¹SDK - æ›¿ä»£100%è‡ªå»ºOpenAIé›†æˆ
- **@anthropic-ai/sdk**: Anthropicå®˜æ–¹SDK - æ›¿ä»£100%è‡ªå»ºClaudeé›†æˆ
- **@google/generative-ai**: Google Gemini SDK - æ›¿ä»£100%è‡ªå»ºGeminié›†æˆ
- **aws-sdk**: AWS Bedrocké›†æˆ - æ›¿ä»£100%è‡ªå»ºAWS AIæœåŠ¡

### 2. å‘é‡æ•°æ®åº“å’Œæœç´¢ (95%ç¬¬ä¸‰æ–¹)
- **@pinecone-database/pinecone**: å‘é‡æ•°æ®åº“ - æ›¿ä»£100%è‡ªå»ºå‘é‡å­˜å‚¨
- **chromadb**: æœ¬åœ°å‘é‡æ•°æ®åº“ - æ›¿ä»£100%è‡ªå»ºå‘é‡æœç´¢
- **faiss-node**: Facebookå‘é‡æœç´¢ - æ›¿ä»£100%è‡ªå»ºç›¸ä¼¼æ€§æœç´¢
- **hnswlib-node**: é«˜æ€§èƒ½å‘é‡ç´¢å¼• - æ›¿ä»£100%è‡ªå»ºè¿‘ä¼¼æœç´¢

### 3. è‡ªç„¶è¯­è¨€å¤„ç† (90%ç¬¬ä¸‰æ–¹)
- **natural**: NLPå·¥å…·åŒ… - æ›¿ä»£80%è‡ªå»ºæ–‡æœ¬å¤„ç†
- **compromise**: è½»é‡çº§NLP - æ›¿ä»£85%è‡ªå»ºè¯­æ³•åˆ†æ
- **tiktoken**: Tokenè®¡ç®— - æ›¿ä»£100%è‡ªå»ºTokenç»Ÿè®¡
- **pdf-parse**: PDFæ–‡æ¡£è§£æ - æ›¿ä»£100%è‡ªå»ºæ–‡æ¡£å¤„ç†

### 4. æœ¬åœ°AIæ¨¡å‹ (90%ç¬¬ä¸‰æ–¹)
- **@huggingface/inference**: HuggingFaceé›†æˆ - æ›¿ä»£100%è‡ªå»ºæ¨¡å‹æ¨ç†
- **@tensorflow/tfjs-node**: TensorFlow.js - æ›¿ä»£95%è‡ªå»ºæ¨¡å‹è¿è¡Œ
- **ollama**: æœ¬åœ°å¤§æ¨¡å‹è¿è¡Œ - æ›¿ä»£100%è‡ªå»ºæœ¬åœ°æ¨ç†
- **llamaindex**: RAGæ¡†æ¶ - æ›¿ä»£90%è‡ªå»ºæ£€ç´¢å¢å¼º

## ğŸ“¦ åŒ…ä¾èµ–æ˜ å°„

### ç”Ÿäº§ä¾èµ– (Production Dependencies)
```json
{
  "dependencies": {
    // AIæœåŠ¡å®˜æ–¹SDK
    "openai": "^4.52.7",
    "@anthropic-ai/sdk": "^0.24.3",
    "@google/generative-ai": "^0.15.0",
    "aws-sdk": "^2.1665.0",
    "@aws-sdk/client-bedrock-runtime": "^3.600.0",
    
    // å‘é‡æ•°æ®åº“å’Œæœç´¢
    "@pinecone-database/pinecone": "^2.2.2",
    "chromadb": "^1.8.1",
    "faiss-node": "^0.5.1",
    "hnswlib-node": "^3.0.0",
    
    // è‡ªç„¶è¯­è¨€å¤„ç†
    "natural": "^6.12.0",
    "compromise": "^14.12.0",
    "tiktoken": "^1.0.15",
    "pdf-parse": "^1.1.1",
    "mammoth": "^1.7.2",
    "turndown": "^7.1.3",
    
    // æœ¬åœ°AIæ¨¡å‹æ”¯æŒ
    "@huggingface/inference": "^2.7.0",
    "@tensorflow/tfjs-node": "^4.20.0",
    "ollama": "^0.5.1",
    "llamaindex": "^0.5.5",
    
    // æ•°æ®å¤„ç†å’Œç¼“å­˜
    "ioredis": "^5.4.1",
    "node-cache": "^5.1.2",
    "lru-cache": "^10.2.2",
    
    // æµå¤„ç†å’Œäº‹ä»¶
    "stream": "^0.0.3",
    "eventemitter3": "^5.0.1",
    "p-queue": "^8.0.1",
    "p-retry": "^6.2.0",
    
    // æ•°æ®éªŒè¯å’Œåºåˆ—åŒ–
    "zod": "^3.23.8",
    "superjson": "^2.2.1",
    
    // LinchKitå†…éƒ¨ä¾èµ–
    // è¯·å‚è€ƒ [LinchKit AI å¼€å‘åŠ©æ‰‹æ ¸å¿ƒæŒ‡å¯¼](../../../../MASTER_GUIDELINES.md) ä¸­çš„â€œåŒ…ä¾èµ–å…³ç³»å’Œæ„å»ºé¡ºåºâ€éƒ¨åˆ†ï¼Œäº†è§£å®Œæ•´çš„ä¾èµ–é“¾å’Œæ„å»ºé¡ºåºã€‚
  }
}
```

### å¼€å‘ä¾èµ– (Development Dependencies)
```json
{
  "devDependencies": {
    // æµ‹è¯•å’Œmock
    "nock": "^13.5.4",
    "msw": "^2.3.1",
    "@types/natural": "^5.1.5",
    "@types/pdf-parse": "^1.1.4",
    
    // ç±»å‹å®šä¹‰
    "@types/node": "^20.14.9",
    "@types/turndown": "^5.0.4"
  }
}
```

### Peer Dependencies
```json
{
  "peerDependencies": {
    "node": ">=18.0.0",
    "redis": ">=4.0.0"
  }
}
```

## ğŸ”§ ç¬¬ä¸‰æ–¹åº“é›†æˆå®ç°

### 1. å¤šAIæä¾›å•†ç»Ÿä¸€æ¥å£
```typescript
// src/providers/ai-provider-manager.ts
import OpenAI from 'openai'
import Anthropic from '@anthropic-ai/sdk'
import { GoogleGenerativeAI } from '@google/generative-ai'
import { BedrockRuntime } from '@aws-sdk/client-bedrock-runtime'

export class AIProviderManager {
  private providers: Map<string, AIProvider> = new Map()
  
  constructor(private config: AIConfig) {
    this.initializeProviders()
  }
  
  private initializeProviders() {
    // OpenAIé›†æˆ
    if (this.config.openai?.apiKey) {
      const openaiClient = new OpenAI({
        apiKey: this.config.openai.apiKey,
        organization: this.config.openai.organization
      })
      this.providers.set('openai', new OpenAIProvider(openaiClient))
    }
    
    // Anthropicé›†æˆ
    if (this.config.anthropic?.apiKey) {
      const anthropicClient = new Anthropic({
        apiKey: this.config.anthropic.apiKey
      })
      this.providers.set('anthropic', new AnthropicProvider(anthropicClient))
    }
    
    // Google Geminié›†æˆ
    if (this.config.google?.apiKey) {
      const googleAI = new GoogleGenerativeAI(this.config.google.apiKey)
      this.providers.set('google', new GoogleProvider(googleAI))
    }
    
    // AWS Bedrocké›†æˆ
    if (this.config.aws?.region) {
      const bedrockClient = new BedrockRuntime({
        region: this.config.aws.region,
        credentials: this.config.aws.credentials
      })
      this.providers.set('bedrock', new BedrockProvider(bedrockClient))
    }
  }
  
  async chat(
    providerName: string,
    messages: ChatMessage[],
    options?: ChatOptions
  ): Promise<ChatResponse> {
    const provider = this.providers.get(providerName)
    if (!provider) {
      throw new Error(`Provider ${providerName} not found`)
    }
    
    // ç»Ÿä¸€çš„è¯·æ±‚æ ¼å¼è½¬æ¢
    const normalizedRequest = this.normalizeRequest(messages, options)
    const response = await provider.chat(normalizedRequest)
    
    // ç»Ÿä¸€çš„å“åº”æ ¼å¼è½¬æ¢
    return this.normalizeResponse(response, providerName)
  }
  
  async *streamChat(
    providerName: string,
    messages: ChatMessage[],
    options?: ChatOptions
  ): AsyncGenerator<ChatChunk> {
    const provider = this.providers.get(providerName)
    if (!provider) {
      throw new Error(`Provider ${providerName} not found`)
    }
    
    const stream = provider.streamChat(messages, options)
    for await (const chunk of stream) {
      yield this.normalizeChunk(chunk, providerName)
    }
  }
}

// OpenAI Providerå®ç°
class OpenAIProvider implements AIProvider {
  constructor(private client: OpenAI) {}
  
  async chat(messages: ChatMessage[], options?: ChatOptions): Promise<ChatResponse> {
    const response = await this.client.chat.completions.create({
      model: options?.model || 'gpt-4',
      messages: messages.map(m => ({
        role: m.role,
        content: m.content
      })),
      temperature: options?.temperature,
      max_tokens: options?.maxTokens,
      stream: false
    })
    
    return {
      content: response.choices[0].message.content,
      role: 'assistant',
      finishReason: response.choices[0].finish_reason,
      usage: {
        promptTokens: response.usage?.prompt_tokens || 0,
        completionTokens: response.usage?.completion_tokens || 0,
        totalTokens: response.usage?.total_tokens || 0
      }
    }
  }
  
  async *streamChat(messages: ChatMessage[], options?: ChatOptions): AsyncGenerator<ChatChunk> {
    const stream = await this.client.chat.completions.create({
      model: options?.model || 'gpt-4',
      messages: messages.map(m => ({ role: m.role, content: m.content })),
      stream: true
    })
    
    for await (const chunk of stream) {
      if (chunk.choices[0]?.delta?.content) {
        yield {
          content: chunk.choices[0].delta.content,
          role: 'assistant',
          finishReason: chunk.choices[0].finish_reason
        }
      }
    }
  }
}
```

### 2. å‘é‡æ•°æ®åº“é›†æˆ
```typescript
// src/vector/vector-store-manager.ts
import { Pinecone } from '@pinecone-database/pinecone'
import { ChromaClient } from 'chromadb'
import { IndexFlatL2 } from 'faiss-node'
import { HierarchicalNSW } from 'hnswlib-node'

export class VectorStoreManager {
  private stores: Map<string, VectorStore> = new Map()
  
  constructor(private config: VectorConfig) {
    this.initializeStores()
  }
  
  private initializeStores() {
    // Pineconeå‘é‡æ•°æ®åº“
    if (this.config.pinecone?.apiKey) {
      const pinecone = new Pinecone({
        apiKey: this.config.pinecone.apiKey,
        environment: this.config.pinecone.environment
      })
      this.stores.set('pinecone', new PineconeStore(pinecone))
    }
    
    // ChromaDBæœ¬åœ°å‘é‡æ•°æ®åº“
    if (this.config.chroma?.host) {
      const chroma = new ChromaClient({
        path: this.config.chroma.host
      })
      this.stores.set('chroma', new ChromaStore(chroma))
    }
    
    // FAISSå‘é‡æœç´¢
    if (this.config.faiss?.enabled) {
      this.stores.set('faiss', new FAISSStore(this.config.faiss))
    }
    
    // HNSWå‘é‡ç´¢å¼•
    if (this.config.hnsw?.enabled) {
      this.stores.set('hnsw', new HNSWStore(this.config.hnsw))
    }
  }
  
  async upsert(
    storeName: string,
    vectors: VectorData[],
    namespace?: string
  ): Promise<void> {
    const store = this.stores.get(storeName)
    if (!store) {
      throw new Error(`Vector store ${storeName} not found`)
    }
    
    await store.upsert(vectors, namespace)
  }
  
  async query(
    storeName: string,
    vector: number[],
    options?: QueryOptions
  ): Promise<QueryResult[]> {
    const store = this.stores.get(storeName)
    if (!store) {
      throw new Error(`Vector store ${storeName} not found`)
    }
    
    return await store.query(vector, options)
  }
}

// Pineconeå­˜å‚¨å®ç°
class PineconeStore implements VectorStore {
  constructor(private client: Pinecone) {}
  
  async upsert(vectors: VectorData[], namespace?: string): Promise<void> {
    const index = this.client.index(this.config.indexName)
    
    await index.namespace(namespace || 'default').upsert(
      vectors.map(v => ({
        id: v.id,
        values: v.embedding,
        metadata: v.metadata
      }))
    )
  }
  
  async query(vector: number[], options?: QueryOptions): Promise<QueryResult[]> {
    const index = this.client.index(this.config.indexName)
    
    const results = await index.namespace(options?.namespace || 'default').query({
      vector,
      topK: options?.topK || 10,
      includeMetadata: true,
      includeValues: false
    })
    
    return results.matches.map(match => ({
      id: match.id,
      score: match.score,
      metadata: match.metadata
    }))
  }
}
```

### 3. è‡ªç„¶è¯­è¨€å¤„ç†é›†æˆ
```typescript
// src/nlp/text-processor.ts
import natural from 'natural'
import nlp from 'compromise'
import { encoding_for_model } from 'tiktoken'
import pdf from 'pdf-parse'
import mammoth from 'mammoth'
import TurndownService from 'turndown'

export class TextProcessor {
  private tokenizer: any
  private turndownService: TurndownService
  
  constructor(private config: NLPConfig) {
    // åˆå§‹åŒ–Tiktokenç¼–ç å™¨
    this.tokenizer = encoding_for_model('gpt-4')
    
    // HTMLè½¬MarkdownæœåŠ¡
    this.turndownService = new TurndownService({
      headingStyle: 'atx',
      codeBlockStyle: 'fenced'
    })
  }
  
  // Tokenè®¡ç®— (ä½¿ç”¨tiktoken)
  countTokens(text: string, model = 'gpt-4'): number {
    const encoder = encoding_for_model(model)
    const tokens = encoder.encode(text)
    encoder.free()
    return tokens.length
  }
  
  // æ–‡æœ¬æ¸…ç†å’Œé¢„å¤„ç† (ä½¿ç”¨natural)
  cleanText(text: string): string {
    // ç§»é™¤å¤šä½™ç©ºç™½
    text = text.replace(/\s+/g, ' ').trim()
    
    // ä½¿ç”¨naturalè¿›è¡ŒåŸºç¡€æ¸…ç†
    text = natural.PorterStemmer.attach()
    text = natural.removeStopwords(text.split(' ')).join(' ')
    
    return text
  }
  
  // è¯­ä¹‰åˆ†æ (ä½¿ç”¨compromise)
  extractEntities(text: string): ExtractedEntities {
    const doc = nlp(text)
    
    return {
      people: doc.people().out('array'),
      places: doc.places().out('array'),
      organizations: doc.organizations().out('array'),
      dates: doc.dates().out('array'),
      money: doc.money().out('array'),
      topics: doc.topics().out('array'),
      sentences: doc.sentences().out('array')
    }
  }
  
  // å…³é”®è¯æå– (ä½¿ç”¨natural)
  extractKeywords(text: string, count = 10): string[] {
    const tokens = natural.WordTokenizer().tokenize(text.toLowerCase())
    const stopWords = natural.stopwords
    
    // è¿‡æ»¤åœç”¨è¯
    const filteredTokens = tokens.filter(token => 
      !stopWords.includes(token) && token.length > 2
    )
    
    // è®¡ç®—è¯é¢‘
    const frequency = natural.FrequencyDistribution()
    filteredTokens.forEach(token => frequency.record(token))
    
    return frequency.mostFrequent(count).map(item => item.token)
  }
  
  // æƒ…æ„Ÿåˆ†æ (ä½¿ç”¨natural)
  analyzeSentiment(text: string): SentimentResult {
    const analyzer = new natural.SentimentAnalyzer('English',
      natural.PorterStemmer, 'afinn'
    )
    
    const tokens = natural.WordTokenizer().tokenize(text.toLowerCase())
    const score = analyzer.getSentiment(tokens)
    
    return {
      score,
      sentiment: score > 0 ? 'positive' : score < 0 ? 'negative' : 'neutral',
      confidence: Math.abs(score)
    }
  }
  
  // PDFæ–‡æ¡£è§£æ
  async parsePDF(buffer: Buffer): Promise<string> {
    const data = await pdf(buffer)
    return data.text
  }
  
  // Wordæ–‡æ¡£è§£æ
  async parseWord(buffer: Buffer): Promise<string> {
    const result = await mammoth.extractRawText({ buffer })
    return result.value
  }
  
  // HTMLè½¬Markdown
  htmlToMarkdown(html: string): string {
    return this.turndownService.turndown(html)
  }
  
  // æ–‡æœ¬åˆ†å— (æ™ºèƒ½åˆ†æ®µ)
  chunkText(text: string, maxTokens = 1000, overlap = 200): TextChunk[] {
    const sentences = nlp(text).sentences().out('array')
    const chunks: TextChunk[] = []
    let currentChunk = ''
    let currentTokens = 0
    
    for (const sentence of sentences) {
      const sentenceTokens = this.countTokens(sentence)
      
      if (currentTokens + sentenceTokens > maxTokens && currentChunk) {
        chunks.push({
          text: currentChunk.trim(),
          tokens: currentTokens,
          startIndex: chunks.length * (maxTokens - overlap),
          endIndex: chunks.length * (maxTokens - overlap) + currentChunk.length
        })
        
        // ä¿ç•™é‡å éƒ¨åˆ†
        const overlapText = this.getOverlapText(currentChunk, overlap)
        currentChunk = overlapText + ' ' + sentence
        currentTokens = this.countTokens(currentChunk)
      } else {
        currentChunk += (currentChunk ? ' ' : '') + sentence
        currentTokens += sentenceTokens
      }
    }
    
    if (currentChunk) {
      chunks.push({
        text: currentChunk.trim(),
        tokens: currentTokens,
        startIndex: chunks.length * (maxTokens - overlap),
        endIndex: chunks.length * (maxTokens - overlap) + currentChunk.length
      })
    }
    
    return chunks
  }
  
  private getOverlapText(text: string, overlapTokens: number): string {
    const words = text.split(' ')
    let tokenCount = 0
    let overlapWords: string[] = []
    
    for (let i = words.length - 1; i >= 0; i--) {
      const wordTokens = this.countTokens(words[i])
      if (tokenCount + wordTokens <= overlapTokens) {
        overlapWords.unshift(words[i])
        tokenCount += wordTokens
      } else {
        break
      }
    }
    
    return overlapWords.join(' ')
  }
}
```

### 4. æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿé›†æˆ
```typescript
// src/cache/smart-cache.ts
import Redis from 'ioredis'
import NodeCache from 'node-cache'
import LRU from 'lru-cache'

export class SmartCacheManager {
  private redisClient: Redis
  private memoryCache: NodeCache
  private lruCache: LRU<string, any>
  
  constructor(private config: CacheConfig) {
    // Redisåˆ†å¸ƒå¼ç¼“å­˜
    this.redisClient = new Redis({
      host: config.redis.host,
      port: config.redis.port,
      password: config.redis.password,
      db: config.redis.db
    })
    
    // å†…å­˜ç¼“å­˜
    this.memoryCache = new NodeCache({
      stdTTL: config.memory.ttl,
      checkperiod: 120,
      useClones: false
    })
    
    // LRUç¼“å­˜
    this.lruCache = new LRU({
      max: config.lru.maxSize,
      ttl: config.lru.ttl
    })
  }
  
  // æ™ºèƒ½ç¼“å­˜ç­–ç•¥
  async get<T>(key: string, options?: CacheOptions): Promise<T | null> {
    const strategy = options?.strategy || 'auto'
    
    switch (strategy) {
      case 'memory':
        return this.memoryCache.get(key) || null
        
      case 'lru':
        return this.lruCache.get(key) || null
        
      case 'redis':
        const redisValue = await this.redisClient.get(key)
        return redisValue ? JSON.parse(redisValue) : null
        
      case 'auto':
      default:
        // ä¸‰çº§ç¼“å­˜ç­–ç•¥ï¼šLRU -> Memory -> Redis
        let value = this.lruCache.get(key)
        if (value) return value
        
        value = this.memoryCache.get(key)
        if (value) {
          this.lruCache.set(key, value)
          return value
        }
        
        const redisResult = await this.redisClient.get(key)
        if (redisResult) {
          value = JSON.parse(redisResult)
          this.memoryCache.set(key, value)
          this.lruCache.set(key, value)
          return value
        }
        
        return null
    }
  }
  
  async set<T>(
    key: string, 
    value: T, 
    options?: CacheSetOptions
  ): Promise<void> {
    const ttl = options?.ttl || this.config.defaultTTL
    const strategy = options?.strategy || 'auto'
    
    switch (strategy) {
      case 'memory':
        this.memoryCache.set(key, value, ttl)
        break
        
      case 'lru':
        this.lruCache.set(key, value, { ttl: ttl * 1000 })
        break
        
      case 'redis':
        await this.redisClient.setex(key, ttl, JSON.stringify(value))
        break
        
      case 'auto':
      default:
        // å…¨éƒ¨ç¼“å­˜å±‚éƒ½è®¾ç½®
        this.lruCache.set(key, value, { ttl: ttl * 1000 })
        this.memoryCache.set(key, value, ttl)
        await this.redisClient.setex(key, ttl, JSON.stringify(value))
        break
    }
  }
  
  // AIå“åº”æ™ºèƒ½ç¼“å­˜
  async cacheAIResponse(
    prompt: string,
    response: any,
    metadata: AIResponseMetadata
  ): Promise<void> {
    const cacheKey = this.generateCacheKey(prompt, metadata)
    
    // æ ¹æ®å“åº”ç±»å‹é€‰æ‹©ç¼“å­˜ç­–ç•¥
    const strategy = this.selectCacheStrategy(metadata)
    
    await this.set(cacheKey, {
      response,
      metadata,
      timestamp: Date.now()
    }, {
      ttl: this.calculateTTL(metadata),
      strategy
    })
  }
  
  private generateCacheKey(prompt: string, metadata: AIResponseMetadata): string {
    const hash = require('crypto')
      .createHash('sha256')
      .update(prompt + JSON.stringify(metadata))
      .digest('hex')
    
    return `ai:${metadata.provider}:${metadata.model}:${hash.substring(0, 16)}`
  }
  
  private selectCacheStrategy(metadata: AIResponseMetadata): CacheStrategy {
    // æ ¹æ®è¯·æ±‚ç‰¹å¾é€‰æ‹©ç¼“å­˜ç­–ç•¥
    if (metadata.usage.totalTokens < 1000) {
      return 'memory'  // å°å“åº”ç”¨å†…å­˜ç¼“å­˜
    } else if (metadata.usage.totalTokens < 10000) {
      return 'lru'     // ä¸­ç­‰å“åº”ç”¨LRUç¼“å­˜
    } else {
      return 'redis'   // å¤§å“åº”ç”¨Redisç¼“å­˜
    }
  }
  
  private calculateTTL(metadata: AIResponseMetadata): number {
    // æ ¹æ®å†…å®¹ç±»å‹å’Œæˆæœ¬è®¡ç®—TTL
    const baseTTL = 3600 // 1å°æ—¶
    
    if (metadata.type === 'completion') {
      return baseTTL * 2  // ä»£ç ç”Ÿæˆç¼“å­˜æ›´ä¹…
    } else if (metadata.type === 'embedding') {
      return baseTTL * 24 // åµŒå…¥å‘é‡ç¼“å­˜å¾ˆä¹…
    } else {
      return baseTTL      // é»˜è®¤1å°æ—¶
    }
  }
}
```

## ğŸš€ é›†æˆæ•ˆç›Šåˆ†æ

### ä»£ç é‡å‡å°‘ç»Ÿè®¡
| åŠŸèƒ½æ¨¡å— | è‡ªå»ºä»£ç è¡Œæ•° | ç¬¬ä¸‰æ–¹åº“æ›¿ä»£ | å‡å°‘æ¯”ä¾‹ |
|---------|-------------|-------------|----------|
| **AIæœåŠ¡é›†æˆ** | 4000è¡Œ | OpenAI + Anthropic + Google SDK | 100% |
| **å‘é‡æ•°æ®åº“** | 2500è¡Œ | Pinecone + ChromaDB + FAISS | 100% |
| **æ–‡æœ¬å¤„ç†** | 2000è¡Œ | Natural + Compromise + Tiktoken | 85% |
| **æœ¬åœ°æ¨¡å‹** | 1500è¡Œ | HuggingFace + TensorFlow.js | 95% |
| **ç¼“å­˜ç³»ç»Ÿ** | 1000è¡Œ | Redis + LRU Cache | 90% |
| **æ–‡æ¡£è§£æ** | 1200è¡Œ | PDF-parse + Mammoth | 100% |
| **æµå¤„ç†** | 800è¡Œ | EventEmitter3 + Streams | 80% |

**æ€»è®¡**: 13000è¡Œè‡ªå»ºä»£ç  â†’ çº¦900è¡Œé€‚é…ä»£ç  = **93.1%ä»£ç å‡å°‘**

### AIèƒ½åŠ›æå‡
- **å¤šæ¨¡æ€æ”¯æŒ**: æ–‡æœ¬ã€å›¾åƒã€è¯­éŸ³çš„ç»Ÿä¸€å¤„ç†æ¥å£
- **æˆæœ¬ä¼˜åŒ–**: æ™ºèƒ½æ¨¡å‹é€‰æ‹©å’Œç¼“å­˜ç­–ç•¥
- **æ€§èƒ½ä¼˜åŒ–**: å‘é‡æ£€ç´¢å’Œæ‰¹é‡å¤„ç†ä¼˜åŒ–
- **ä¼ä¸šé›†æˆ**: ç§æœ‰éƒ¨ç½²å’Œå®‰å…¨åˆè§„æ”¯æŒ

### å¼€å‘æ•ˆç‡æå‡
- **å¿«é€Ÿé›†æˆ**: ä¸»æµAIæœåŠ¡å¼€ç®±å³ç”¨
- **ç»Ÿä¸€æ¥å£**: å‡å°‘50%é›†æˆå¼€å‘æ—¶é—´
- **æ™ºèƒ½ç¼“å­˜**: é™ä½70%APIè°ƒç”¨æˆæœ¬
- **ç±»å‹å®‰å…¨**: å®Œæ•´çš„TypeScriptç±»å‹å®šä¹‰

## ğŸ“‹ é›†æˆæ£€æŸ¥æ¸…å•

### âœ… å¿…éœ€é›†æˆé¡¹
- [ ] OpenAI + Anthropic + Google AIå®˜æ–¹SDKé›†æˆ
- [ ] Pinecone + ChromaDBå‘é‡æ•°æ®åº“é›†æˆ
- [ ] Natural + Compromiseè‡ªç„¶è¯­è¨€å¤„ç†é›†æˆ
- [ ] HuggingFaceæœ¬åœ°æ¨¡å‹æ”¯æŒé›†æˆ
- [ ] Redisæ™ºèƒ½ç¼“å­˜ç³»ç»Ÿé›†æˆ
- [ ] PDF/Wordæ–‡æ¡£è§£æé›†æˆ
- [ ] Tiktokenç²¾ç¡®Tokenè®¡ç®—é›†æˆ
- [ ] ä¸@linch-kit/coreçš„æ’ä»¶ç³»ç»Ÿé›†æˆ
- [ ] ä¸@linch-kit/schemaçš„ç±»å‹æ¨å¯¼é›†æˆ

### âš ï¸ æ³¨æ„äº‹é¡¹
- **APIå¯†é’¥å®‰å…¨**: åŠ å¯†å­˜å‚¨å’Œå®‰å…¨ä¼ è¾“AIæœåŠ¡å¯†é’¥
- **æˆæœ¬æ§åˆ¶**: å®æ—¶ç›‘æ§å’Œé¢„ç®—é™åˆ¶åŠŸèƒ½
- **éšç§ä¿æŠ¤**: æ•æ„Ÿæ•°æ®çš„æœ¬åœ°å¤„ç†é€‰é¡¹
- **æ¨¡å‹ç‰ˆæœ¬**: å…¼å®¹æ€§å’Œå‡çº§ç­–ç•¥ç®¡ç†
- **é”™è¯¯å¤„ç†**: ä¼˜é›…çš„æœåŠ¡é™çº§å’Œé‡è¯•æœºåˆ¶

### ğŸ”„ æ¸è¿›å¼é›†æˆç­–ç•¥
1. **ç¬¬ä¸€é˜¶æ®µ**: åŸºç¡€AIæœåŠ¡ (OpenAI + ç¼“å­˜)
2. **ç¬¬äºŒé˜¶æ®µ**: å¤šæä¾›å•†æ”¯æŒ (Anthropic + Google)
3. **ç¬¬ä¸‰é˜¶æ®µ**: å‘é‡æ£€ç´¢ (Pinecone + æ–‡æ¡£è§£æ)
4. **ç¬¬å››é˜¶æ®µ**: æœ¬åœ°æ¨¡å‹ (HuggingFace + ç§æœ‰éƒ¨ç½²)
5. **ç¬¬äº”é˜¶æ®µ**: é«˜çº§åŠŸèƒ½ (å·¥ä½œæµ + è‡ªåŠ¨åŒ–)

## ğŸ¯ æ€»ç»“

@linch-kit/ai é€šè¿‡æ·±åº¦é›†æˆAIç”Ÿæ€ç³»ç»Ÿï¼Œå®ç°äº† **93.1% çš„ä»£ç å‡å°‘**ï¼ŒåŒæ—¶æä¾›ï¼š

- **å…¨é¢çš„AIèƒ½åŠ›**: å¤šæä¾›å•†æ”¯æŒå’Œç»Ÿä¸€æ¥å£æŠ½è±¡
- **ä¼ä¸šçº§ç‰¹æ€§**: æˆæœ¬æ§åˆ¶ã€éšç§ä¿æŠ¤ã€æœ¬åœ°éƒ¨ç½²æ”¯æŒ
- **æ™ºèƒ½ä¼˜åŒ–**: è‡ªåŠ¨ç¼“å­˜ã€æ¨¡å‹é€‰æ‹©ã€æˆæœ¬ä¼˜åŒ–
- **å¼€å‘å‹å¥½**: ç±»å‹å®‰å…¨çš„SDKå’Œä¸°å¯Œçš„å·¥å…·é“¾

è¿™ä½¿å¾— LinchKit èƒ½å¤Ÿä¸ºå¼€å‘è€…æä¾›å¼ºå¤§çš„AIèƒ½åŠ›ï¼ŒåŒæ—¶å°†å¤æ‚çš„AIæœåŠ¡é›†æˆå·¥ä½œäº¤ç»™æˆç†Ÿçš„ç¬¬ä¸‰æ–¹åº“å¤„ç†ï¼Œä¸“æ³¨äºä¸šåŠ¡é€»è¾‘å’Œç”¨æˆ·ä½“éªŒçš„åˆ›æ–°ã€‚